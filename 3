df=df.iloc[125400:129400,:]

df.isnull().any()

df.isnull().sum()


df=df.dropna()


df.isnull().sum()

df["Invoice"].nunique()

df.groupby("Country")["Invoice"].nunique()

df["TotalPrice"]=df["Quantity"]*df["Price"]

df.sort_values(by="TotalPrice",ascending=False)

df.groupby(by="Customer ID")[["TotalPrice"]].sum().sort_values(by="TotalPrice",ascending=False)

df[df["Customer ID"]==13694.0].count()[["Invoice"]]


df_new_cluster=df_cluster.iloc[:,1:]

df_selected_cluster=df_new_cluster.iloc[42482:43476,:]

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

numerical_features = df_selected_cluster
scaler = StandardScaler()
numerical_features_scaled = scaler.fit_transform(numerical_features)


ca = PCA(n_components=2)
pca_result = pca.fit_transform(numerical_features_scaled)

pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])

plt.figure(figsize=(10, 6))
plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.5)
plt.title('2D PCA Visualization')
plt.xlabel('Principal Component 1 (PC1)')
plt.ylabel('Principal Component 2 (PC2)')
plt.grid(True)
plt.show()

from sklearn.cluster import KMeans
from scipy.spatial.distance import euclidean
index_11=pca_result[11]
index_832=pca_result[832]

most_sig_11=index_11[0]
second_sig_832=index_832[1]

distance=euclidean(index_11,index_832)

kmeans=KMeans(n_clusters=5,random_state=0)
kmeans.fit(pca_result)
labels=kmeans.labels_

cluster_label_11=labels[11]
cluster_label_832=labels[832]
print(most_sig_11)
print(second_sig_832)
print(distance)
print(cluster_label_11)
print(cluster_label_832)




# -*- coding: utf-8 -*-
"""Working with large data - Lecture - 311024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oGqHWTbBjKl_jUySJdl0OQRoIAPmGuzV

<a id="0"></a>
# Working with large data in Python
This file covers following topics:

## 1. [What is HDF and why to use it?](#1)
###     1.1. [Create simple HDF5 file](#1.1)
###     1.2. [Read simple HDF5 file](#1.2)
###     1.3. [Create large HDF5 file](#1.3)
## 2. [Python generator for lazy load of HDF5 file](#2)
## 3. [Working with Pandas](#3)
###     3.1 [Plot support in Pandas](#3.1)
###     3.2 [Pandas function examples](#3.2)
###     3.3 [Pandas slicing examples](#3.3)
###     3.4 [Reading and exporting DataFrame](#3.4)
## 4. [Dask](#4)
###     4.1 [Starting local cluster](#4.1)
###     4.2 [Dask Array](#4.2)
###     4.3 [Dask DataFrame](#4.3)
###     4.4 [Computing with Dask](#4.4)
###     4.5 [Custom lazy functions](#4.5)

#### Requirements
#### python 3.6.8<p>
dask==2021.10.0<p>
h5py==2.10.0<p>
matplotlib==3.4.3<p>
numpy==1.21.2<p>
pandas==1.3.4<p>
#### or python >= 3.9.7:
dask==2021.12.0<p>
h5py==3.6.0<p>
matplotlib==3.5.0<p>
numpy==1.21.2<p>
pandas==1.3.5<p>
<p> Installation can be done (<strong>with admin rights!</strong>) by: <strong><em>pip install -r requirements.txt</em></strong><p><em>Can be run in jupyter enviroment</em>
<p> Due to some additional requirements by nested lib it is required to finish installation of with following code:
<p><strong><em>python -m pip install "dask[complete]" </em></strong>
<p><strong><em>conda install pytables</em></strong>
<p><strong><em>conda install openpyxl</em></strong>
<p> Requirements of any jupyter notebook can be extracted by installing following packages:<strong><em><p>pip install pipreqs<p>
pip install nbconvert<p></em></strong> and calling from console following lines:<strong><em><p>jupyter nbconvert --output-dir="./reqs" --to script myntb.ipynb<p>cd reqs<p>pipreqs</em></strong>
"""

# Commented out IPython magic to ensure Python compatibility.
# Import required libraries for entire jupyter notebook
import h5py
import time
import numpy as np
import pandas as pd
# Magic has to be used before importing matplotlib.
# For static images use inline, for interactive use notebook or widget
# %matplotlib inline
import matplotlib.pyplot as plt
# Dask related imports
import dask
import dask.array as da
import dask.dataframe as dd
from dask.distributed import LocalCluster, Client

"""This document has interactive linking
 <a id="1"></a>
 # What is HDF and why to use it?
The Hierarchical Data Format version 5 (HDF5), is an open source file format that supports large, complex, heterogeneous data. HDF5 uses a "file directory" like structure that allows you to organize data within the file in many different structured ways, as you might do with files on your computer. The HDF5 format also allows for embedding of metadata making it self-describing. [[1]](https://www.neonscience.org/resources/learning-hub/tutorials/about-hdf5)
<div>
<center><img src="https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/dev-aten/graphics/HDF5-general/whyHDF5.jpg" width="500"/>
</center>
</div><div><center>
<strong>When is HDF5 good to use</strong><a href="https://www.neonscience.org/resources/learning-hub/tutorials/about-hdf5">[1]</a></center>
</div>

<a id="1.1"></a>
# Create simple HDF5 file
HDF5, as i's from the name obvious, is a hierarchical structure, which means it's basically file directory within a file. As seen on image below
<div><center>
<img src="https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/dev-aten/graphics/HDF5-general/hdf5_structure3.jpg" width="500"/>
</center></div><div><center>
<center><strong>An example HDF5 file structure containing data for multiple field sites and also containing various datasets (averaged at different time intervals).</strong> <a href="https://www.neonscience.org/resources/learning-hub/tutorials/about-hdf5">[1]</a></center></div>

Main features of working with file type hdf5:
- It can be compared to working with file system on real hard-drive in OS. Just like on real hard-drive we can create folders which can contain more files.
- These folders in HDF5 we call __groups__.
- And just like on real hard-drive where we can have folders within folders. In hdf5 we can also have nested groups.
- New datasets (files) can be created either by direct indexing (similar to new columns in pandas!) or by preparing empty space with optional settings.
- If dataset is created by direct indexing, its not changable in its dimensions, unlike prepared empty space which can be specified to be resizeable.

For our example we want to create file with following structure:

"""

nrandom_0 = 500 # Number of random values in first dataset
nrandom_1 = 23 # Number of random values in second dataset

# Create and open hdf5 file
with h5py.File("random_small_file.h5", "w") as file:
    # Create first group"
    group0 = file.create_group("Group0")

    # Create nested group
    group1 = group0.create_group("Group1")

    # Create dataset 'Noise' in group1 and input  random values
    group1["Noise"] = np.random.random_sample(nrandom_0)

    # Create dataset 'EverGrowingData' in group1 with specific properties
    dataset_2 = group1.create_dataset("EverGrowingData", shape=(10,10), maxshape=(None, None))

    # Since maxshape was specified, dataset is resizeable
    dataset_2.resize((nrandom_1, 10))
    # Attempt to resize fixed dataset
    # dataset_1 = group1["Noise"].resize(1000, 5)

    # Input data to resized dataset
    dataset_2[:, 1] = 5*np.random.random_sample(nrandom_1)-5

"""[top](#0)
<a id="1.2"></a>
# Read created HDF5 file
As mentioned earlier, hdf5 file acts just like a physical hard-drive. We can read out what is available in what group (folder) in reccurent way. In similar fashion you can step through entire file in your OS.
"""

# Get available groups and datasets
with h5py.File("random_small_file.h5", "r") as file:
    print("Available dataset on root", list(file.keys()))

"""Just like reading out what is in what group, we can read out all properties of stored datasets. These properties are stored in related metadata and therefore its not required to load entire dataset to check for example its shape."""

# Get properties of datasets in "/Group0/Group1"
with h5py.File("random_small_file.h5", "r") as file:
    group1 = file["Group0"]["Group1"]
    print(group1)
    for key in group1.keys():
        print(f"Shape of {key} - {group1[key].shape}")

"""Reading out data is very similar to fetching selected data from pandas.DataFrame. We can use entirely object-based approach."""

# Get data from datasets
with h5py.File("random_small_file.h5", "r") as file:
    group1 = file["/Group0/Group1"]
    sample_data = np.zeros((4,4))
    sample_data[:, 0] = group1["Noise"][:4]
    sample_data[:, 1:] = group1["EverGrowingData"][:4, :3]
print(sample_data)

"""[top](#0)
<a id="1.3"></a>
# Create large HDF5 file
For your personal testing, you can try to create following file with random data. You can try to save (read) the same amount of data to CSV or other file format which you would like to test, but be careful as it might bring your IPyKernel to halt.
### CAREFUL! 3GB FILE WILL BE CREATED upon running the script
Due to server memory reasons, following code is stored in enclosed python file which you can copy to this empty code line on your PC and you can't run this script in virtual playground.

##### File generated by this script will be required for next examples. If you don't want to (or can't) generate file this large, please change the size accordingly to your system
"""

# Copy in from enclosed file
size = 1024 ** 3 # Target bit size per dataset (1GB x 3 Datasets)
nchunk = 500 # Amount of chunks in which should the data be written
dtype = np.float64 # Type of data in datasets


with h5py.File('.random_data.hdf5', 'w') as file:
    # Create dataset0 in root with name "A"
    dset_0 = file.create_dataset('A', shape=(int(size/8),), dtype=dtype)
    # Prepare indexes for chunks
    writes = np.linspace(0, dset_0.shape[0], nchunk, dtype=np.int64)
    writes[-1] = dset_0.shape[0]
    print("Chunk size = %s" %(writes[1]))
    time.sleep(1)

    # Write data to dataset "A" by chunks
    for chunk in range(1, writes.shape[0]):
        chunk_size = int(writes[chunk]-writes[chunk-1])
        dset_0[writes[chunk-1]:writes[chunk]] = 5*np.random.random_sample(chunk_size)-5
    print("Gen of Dataset A finished")

    # Create dataset1 in root with name "B"
    dset_1 = file.create_dataset('B', shape=(int(size/8),), dtype=dtype)
    # Prepare indexes for chunks
    writes = np.linspace(0, dset_1.shape[0], nchunk, dtype=np.int64)
    writes[-1] = dset_1.shape[0]

    # Write data to dataset "B" by chunks
    for chunk in range(1, writes.shape[0]):
        chunk_size = int(writes[chunk]-writes[chunk-1])
        dset_1[writes[chunk-1]:writes[chunk]] = 10*np.random.random_sample(chunk_size)-5
    print("Gen of Dataset B finished")

    # Create dataset2 in root with name "C"
    dset_2 = file.create_dataset('C', shape=(int(size/8),), dtype=dtype)
    # Prepare indexes for chunks
    writes = np.linspace(0, dset_2.shape[0], nchunk, dtype=np.int64)
    writes[-1] = dset_2.shape[0]

    #  Write data to dataset "C" by chunks
    for chunk in range(1, writes.shape[0]):
        chunk_size = int(writes[chunk]-writes[chunk-1])
        dset_2[writes[chunk-1]:writes[chunk]] = -5*np.random.random_sample(chunk_size)-5
    print("Gen of Dataset C finished")
print("All done!")

"""[top](#0)
<a id="2"></a>
# Python generator for lazy load of HDF5 file
Usually the processing of large data files expects your code to lazy load all the data as they are too big to be loaded to RAM all at once. In Python we can use for this task generators, which can heavily optimize the data management.

Generators are coded just like regular function but they also have atleast one yield keyword and are ending with return or with _StopIteration_ exception

To compare the optimization of memory - we can have generator for 4 milion (or even more) random numbers that takes only 24 bytes of RAM, meanwhile prepared array of this size would require more than 100MB.

Web frameworks such as Django, Flask are based entirely on generators! Otherway they would be very slow and resource heavy.
"""

hdf_path = './.random_data.hdf5' # Set path to large data file

def reading_generator(step:int=5000)->np.array:
    # Numpydoc docstring format
    """
    Returns data from file by given step

    Parameters
    -----------
    step: int, optional
        Index step over large data file

    Yields
    -------
    data: np.array
    """
    with h5py.File(hdf_path, "r") as file:
        maxsize = 0
        for dataset in file.keys():
            size = file.get(dataset).shape[0]
            if size > maxsize:
                maxsize = size
        # Prepare empty space
        data = np.zeros((step, len(file.keys())))
        # Get steps
        steps = np.arange(0, maxsize, step, dtype=np.int32)
        steps[-1] = maxsize

        # Iterate over data
        for istep in range(1, steps.shape[0]):
            for pos, dataset in enumerate(file.keys()):
                data[:, pos] = file.get(dataset)[steps[istep-1]:steps[istep]]
            yield data

"""Generator init usually takes some time and therefore its build and first iteration are usually slower."""

# Read data from file using the generator method
start = time.time()
generator = reading_generator()
data = next(generator)
print(f"Elapsed time: {time.time()-start}")
print(data)

"""However, once the generator is properly initialized, its really fast. As we will proof with the following code."""

# Read data from HDF5 using our generator
start = time.time()
data = next(generator)
print(f"HDF5 generator output in {time.time()-start}")
print(data)

# Save data from generator (for csv test)
np.savetxt("gen_output.csv", data)

# Read same data from csv
start = time.time()
data = np.loadtxt("gen_output.csv")
print(f"CSV  output in {time.time()-start}")

"""[top](#0)
<a id="3"></a>
# Working with Pandas
Pandas is a great tool for initial analysis newly obtained data. It introduces data formats __DataFrame__ and __Series__ those resemble working with structured database (SQL). Pandas has many build-in functions with which you can perform basic data analysis such as _nunique, apply, autocorr, cov, ..._. It also features many built in visualization techniques such as _histogram, box plot, data density, ..._ .

"""

# Create pandas Dataframe with user defined columns
df = pd.DataFrame(next(generator), columns=["A", "B", "C"])
df

"""<a id="3.1"></a>
## Plot support in pandas
Pandas has full feature support of <strong>matplotlib.pyplot</strong> package and uses it for its own direct visualization. Plot object Axis of matplotlib package called from within Pandas can be freely passed onto regular matplotlib-like work by simple means of directly naming axis as parametr in Pandas plotting method.


[2] Matplotlib: Python plotting — Matplotlib 3.4.3 documentation, [online]. Available from: https://matplotlib.org/

"""

# Pandas inherits matplotlib plotting methods
fig, ax = plt.subplots(3, 1)
df.plot(ax=ax, subplots=True)
ax[0].set_title("Plotting")
plt.show()

"""#### HISTOGRAM
As was mentioned earlier, Pandas has many basic analysis methods which can be easily utilized. One of such methods is <strong>histogram</strong>.

KEY TAKEAWAYS:
- A histogram is a bar graph-like representation of data that buckets a range of outcomes into columns along the x-axis.
- The y-axis represents the number count or percentage of occurrences in the data for each column and can be used to visualize data distributions.

<em>Since our input data are generated by us, we know how they should be distributed. You can try to change distribution in data generator and compare your results.</em>

[3] Histogram Definition, Investopedia [online]. Available from: https://www.investopedia.com/terms/h/histogram.asp
"""

# Histogram
fig, ax = plt.subplots(1, 3)
df.hist(ax=ax, rwidth=0.9)
plt.show()

"""#### BOXPLOT
Another of build in visual analysis method in pandas is <strong>Boxplot</strong>.

A boxplot is a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”). It can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed. Visual representation of boxplot method is on following image

<div><center>
<img src="https://miro.medium.com/max/700/1*NRlqiZGQdsIyAu0KzP7LaQ.png" width="500" align="center"/>
</center></div><div>
<center><strong>Visual representation of boxplot method with shown probability of data in certain quartile.</strong> <a href="https://medium.com/python-in-plain-english/an-introduction-to-boxplot-charts-a85c13878502">[4]</a></center></div>

[4] A. T. Dang, “An Introduction to Boxplot Charts,” Python in Plain English. Accessed: Jan. 11, 2024. [Online]. Available: https://python.plainenglish.io/an-introduction-to-boxplot-charts-a85c13878502


"""

# Boxplot
fig, ax = plt.subplots(num="Box plots with pandas")
df.boxplot(ax=ax)
plt.show()

"""[top](#0)
<a id="3.2"></a>
## Pandas function examples
Along with many visualization methods, Pandas comes with many different basic numeric data analysis tools aswell.
#### NUNIQUE
Input data can be very diverse, therefore its usually good to check, how many unique values there are in our freshly obtained data. Yet again we can use Pandas for this task - method nunique will count number of distinct elements in specified axis.

<em> Since our data are randomly generated, all of them should be unique.</em>
"""

# Unique numbers in DataFrame
df.nunique()

"""#### AUTOCORRELATION
Autocorrelation is a mathematical representation of the degree of similarity between a given time series and a lagged version of itself over successive time intervals <a href="https://www.investopedia.com/terms/a/autocorrelation.asp">[5]</a>.

KEY TAKEAWAYS:
- Autocorrelation represents the degree of similarity between a given time series and a lagged version of itself over successive time intervals.
- Autocorrelation measures the relationship between a variable's current value and its past values.
- An autocorrelation of +1 represents a perfect positive correlation, while an autocorrelation of negative 1 represents a perfect negative correlation.

<em>Since our data are randomly generated, there should be basically no autocorrelation.</em>

[5] What Is Autocorrelation?, Investopedia [online]. Available from: https://www.investopedia.com/terms/a/autocorrelation.asp

"""

# Autocorrelation of data. Only works with pandas.Series
df["A"].autocorr(lag=2)

"""#### CORRELATION
Correlation shows the strength of a relationship between two variables and is expressed numerically by the correlation coefficient. The correlation coefficient's values range between -1.0 and 1.0. Also known as Pearson's r <a href="https://www.investopedia.com/terms/c/correlation.asp">[6]</a>.

KEY TAKEAWAYS
- Correlation is a statistic that measures the degree to which two variables move in relation to each other.
- Correlation measures association, but doesn't show if x causes y or vice versa - or if the association is caused by a third factor.<br> Or in other words - <span style="color:red"><strong>Correlation does not imply causation</strong></span>.

<em>Related coefficient</em><br>
<strong>Coefficient of determination</strong> tells us, how many % are explained by linear dependence. Beware of assumptions: $N(0,\sigma)$ , sufficient data, etc., so in practice rather use this coefficient as indicative or as a relative indicator.


[6] What Is Correlation in Finance?, Investopedia [online]. Available from: https://www.investopedia.com/terms/c/correlation.asp

"""

# Correlation of columns in DataFrame
df.corr()

# Coefficient of determination
r = df.corr().values[2, 1]
print("r**2", r**2)

"""#### PAIRWISE COVARIANCE
Covariance indicates the relationship of two variables whenever one variable changes. If an increase in one variable results in an increase in the other variable, both variables are said to have a positive covariance <a href="https://en.wikipedia.org/w/index.php?title=Covariance_matrix&oldid=1049991278">[7]</a>. Covariance is basically not normalized correlation which is defined as

$$
\rho_{x,y}=\frac{cov(X,Y)}{\sigma_{x}\sigma_{y}}
$$

The covariance matrix generalizes the notion of variance to multiple dimensions. As an example, the variation in a collection of random points in two-dimensional space cannot be characterized fully by a single number, nor would the variances in the $\bf{x}$ and $\bf{y}$ directions contain all of the necessary information. For this case a $\bf{2}\times\bf{2}$ matrix would be necessary to fully characterize the two-dimensional variation.

<em>Since our data are randomly generated, there should be covariance only with itself</em>


[7] Covariance matrix, 2021. Wikipedia [online]. Available from: https://en.wikipedia.org/w/index.php?title=Covariance_matrix
"""

# Pairwise covariance of columns
df.cov()

"""[top](#0)
<a id="3.3"></a>
## Pandas slicing approaches
We often want to work with subsets of a DataFrame object. There are different ways to accomplish this including: using labels (column headings), numeric ranges, specific x,y index locations or even specific filters.

#### Direct python slicing
We use square brackets <strong>[ ]</strong> (python list selection) to select a subset of a Python object or direct object approach. For example, we can select all data or just specified data from a column named <strong>A</strong> from our DataFrame by name. In these approaches we first select column, then index. Those two methods are performed as follows:
"""

# Slicing DataFrame as a list
df["A"][7:10]

# Slicing DataFrame as an object
df.A[7:10]

"""We can also pass list of column names when slicing DataFrame as a list. Notice, that first two slicings return Series object instead of DataFrame, while slicing of multiple columns returns again DataFrame. <strong>Not all methods in DataFrame are callable on Series and vice versa!</strong>"""

# Slicing DataFrame as a list with multipltiple columns selected
df[["A", "C"]][7:10]

"""#### LOC and ILOC slicing
We can select specific ranges of our data in both the row and column directions using either label or integer-based indexing.
- <strong>loc</strong> is primarily label based indexing. Integers may be used but they are interpreted as a label.
- <strong>iloc</strong> is primarily integer based indexing.

Unlike previous slicing methods, where we first selected column then row, <strong>in loc methods we first select row then column.</strong>
"""

# Slicing by indexes with iloc method
df.iloc[7:10, 2]

# Slicing by names with loc method
df.loc[7:10, "C"]

"""<em>Notice, that unlike all other slicing methods, method <strong>loc</strong> returned more rows. The cause is, that index is interpreted as label, therefore given range is returned entirely and not index-based as we would expect. If index would be based on names, all requested names would be returned.</em>

#### Conditional slicing
We can also select a subset of our data using criteria. For example, we can select all rows that have a <strong>B</strong> value greater than 0 and smaller than 1. Single condition does not have to be put in brackets <strong>( )</strong>, multiple conditions have to.

We can use the syntax below when querying data by criteria from a DataFrame:
- Equals: <strong>==</strong>
- Not equals: <strong>!=</strong>
- Greater than, less than: <strong>></strong> or <strong><</strong>
- Greater than or equal to <strong>>=</strong>
- Less than or equal to <strong><=</strong>
"""

# Slicing with given condition
df[(df["B"]>0) & (df.B<1)]

"""#### Mask
In similar fashion we can obtain binary mask for further data processing directly from Pandas.
"""

# Obtaining mask for given condition
df["A"] > -0.5

"""[top](#0)
<a id="3.4"></a>
## Reading and saving with Pandas
One crucial feature of Pandas is its ability to write and read Excel, CSV, and many other types of files. Functions like the Pandas <em>read_csv()</em> method save you time from the hassle of file managment, which can make your work time more productive. You can use thse functions to save the data and labels from Pandas objects to a file and load them later as Pandas Series or DataFrame instances.

Altho there are many convenient data types and data load approaches which should be used. One of the most commonly used file format across wide broad of industries is still just excel or csv. Therefore we will do here quick speed comparison of reading excel, csv and hdf5 format for the same sample data.

<em>Sample data are fairly small - 1460 rows X 10 columns and contain text, numbers and NaNs</em>

#### Speed of reading data from CSV file
"""

# Reading data from csv (92KB)
# File name: TechCrunchcontinentalUSA.csv
start = time.time()
df = pd.read_csv("TechCrunchcontinentalUSA.csv")
print(f"CSV reading time {time.time()-start}")

"""#### Speed of reading data from Excel file"""

# Reading from excel (73KB)
# File name: TechCrunchcontinentalUSA.xlsx
start = time.time()
df = pd.read_excel("TechCrunchcontinentalUSA.xlsx")
print(f"Excel reading time {time.time()-start}")

"""#### Saving data to hdf5 file
Saving data with pandas is just as simple as reading them. For many most common file formats there is a method to handle all required steps for you.

<strong>Pandas DataFrame to hdf5 approach</strong>

Pandas does not store its DataFrame contents in the same way as we did at the begining of this file - as numbers and meta-data. Instead pandas serializes entire DataFrame object with the use of pickle and save it as binary data. Therefore it can be read again only with pandas! This approach is quite handy for pandas, but quite unfortune for many native hdf5 file system features - e.g.: Getting shape of stored data without reading them or less efficient lazy-load.
"""

# Saving to hdf5
df.to_hdf("TechCrunchcontinentalUSA.h5", "data")

"""#### Speed of reading data from hdf5
As mentioned earlier, data are saved as serialized pandas DataFrame, not numbers. Due to non optimizable variables to c-types reading such object is usually slower.
"""

# Reading from hdf5 (1213KB)
# File name: TechCrunchcontinentalUSA.h5
start = time.time()
df = pd.read_hdf("TechCrunchcontinentalUSA.h5", "data")
print(f"HDF reading time {time.time()-start}")

"""For more examples how to handle files through Pandas do see <a href="https://realpython.com/pandas-read-write-files/">[8]</a>


[8] PYTHON, Real, Pandas: How to Read and Write Files – Real Python. [online]. Available from: https://realpython.com/pandas-read-write-files/

[top](#0)
<a id="4"></a>
# Dask
Dask is a parallel computing library that scales the existing Python ecosystem. Dask provides multi-core and distributed parallel execution on larger-than-memory datasets, which is fairly easy to implement and develop <a href="https://docs.dask.org/en/stable/">[9]</a>. Dask allows you to quickly rewrite your single-core script into multi-core or even distributed system by replacing eg. numpy array with dask array. Dask adjusted methods and data types usually inerhit all methods of their originals, which means if numpy array can perform transposition on itself, so can dask array.

We can think of Dask at a high and a low level

- <strong>High level collections</strong>: Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and Pandas but can operate in parallel on datasets that don’t fit into memory. Dask’s high-level collections are alternatives to NumPy and Pandas for large datasets.</p>

- <strong>Low Level schedulers</strong>: Dask provides dynamic task schedulers that execute task graphs in parallel. These execution engines power the high-level collections mentioned above but can also power custom, user-defined workloads. These schedulers are low-latency (around 1ms) and work hard to run computations in a small memory footprint. Dask’s schedulers are an alternative to direct use of threading or multiprocessing libraries in complex cases or other task scheduling systems like <em>Luigi</em> or <em>IPython parallel</em>.

Thanks to high and low level integration, Dask can really ease the difficulty of parallel computing on programmer. Visualization of common task approach and integration is as follows
<div><center>
<img src="https://docs.dask.org/en/stable/_images/dask-overview.svg" width="500"/>
</center></div><div>
<center><strong>Visual representation of common task management under Dask framework</strong> <a href="https://docs.dask.org/en/stable/">[9]</a>.</center></div>


In case you are interested in more about Dask than is provided on following lines, I highly recommend you their own tutorial available from <a href="https://github.com/dask/dask-tutorial">HERE</a>.


[9] Dask — Dask documentation, [online]. Available from: https://docs.dask.org/en/stable/


<a id="4.1"></a>
## Starting local Cluster
    
Starting a local cluster is task which will be required to perform every time we want to use the Dask framework for parallel computing with connecting local client to it. Starting distributed cluster is slightly more complicated and harder to simulate on single machine, however most of it is also fully automated. <em>In case you want to learn how to start distributed cluster, do see Dask tutorial and documentation.</em>

- For local cluster Dask will use single machine scheduler.
- For distributer cluster Dask will use distributed machine scheduler to which clients can connect and control tasks. All data are fetched upon request and not pre-cached.
"""

# create local cluster
cluster = LocalCluster()

# connect local client to local cluster
client = Client(cluster)

"""<a id="4.2"></a>
## Dask array
Dask Array implements a subset of the NumPy ndarray interface using chunked algorithms, cutting up the large array into many small arrays - so called chunks. This lets us compute on arrays larger than memory using all of our cores. These chunked algorithms are coordinated using Dask task graphs.
<div><center>
<img src="https://upload.wikimedia.org/wikipedia/commons/9/97/Dask-array-black-text.svg" width="500"/>
</center></div><div>
<center><strong>Visual representation of Dask Array in comparison to NumPy Array. Dask Array can contain data larger than available memory</strong> <a href="https://docs.dask.org/en/stable/">[9]</a>.</center></div>

In following example we create Dask Array full of random numbers in dimensions 10000 by 10000. The chunks argument allows us to define into how many small arrays can be big Dask array broken up. In this case, the array can be broken up in 100 pieces, with each piece by dimensions 1000 by 1000. Using the right chunk size will become extremely important in helping you to optimize advanced algorithms.
"""

# Create dask numpy array of size (10000, 10000) with chunk size (1000, 1000)
#As long as we dont use ".compute(), no array will be created in memory"
x = da.random.random((10000, 10000), chunks=(1000, 1000))
x

"""<a id="4.3"></a>
## Dask DataFrame
A Dask DataFrame is a large parallel DataFrame composed of many smaller Pandas DataFrames, split along the index. These Pandas DataFrames may live on disk for larger-than-memory computing on a single machine, or on many different machines in a cluster. One Dask DataFrame operation triggers many operations on the constituent Pandas DataFrames.

<div><center>
<img src="https://docs.dask.org/en/stable/_images/dask-dataframe.svg" width="250"/>
</center></div><div>
<center><strong>Visual representation of Dask DataFrame in comparison with Pandas DataFrame</strong> <a href="https://docs.dask.org/en/stable/">[9]</a>.</center></div>

In following example you will notice that if you try to see the DataFrame by calling it directly with its variable, you will not be able to see any data. This is because Dask is inherently lazy, which means it will not compute anything unless you explicitly tell it to do so by .compute(). You can also call persist() which will save entire DataFrame to your computer’s memory (RAM) and you will be able to see data directly. Make sure that your computer has enough RAM for you to do so!
"""

# Prepare load task
# File name: ./TechCrunchcontinentalUSA.csv
df = dd.read_csv("TechCrunchcontinentalUSA.csv")
df

# Sort from input DataFrame all companies those produce software
df[df.category=="software"][["company", "raisedAmt"]].compute()

"""<a id="4.4"></a>
## Computing with Dask
Dask is lazily evaluated. The result from a computation isn’t computed until you ask for it. Instead, a Dask task graph for the computation is produced.

Anytime you have a Dask object and you want to get the result to your local machine, you have to call method compute(). As was mentioned earlier, Dask supports most of the features of common data types used in data analytics.

#### Example of numpy in Dask
Transposition of a matrix and addition with the use of Dask.
"""

# Calculating with prepared matrix
# Specify operation (equation)
y = x + x.T

# Compute operation
y.compute()

"""#### Example of dataframe
DataFrame operations can be nicely chained for compact compute call. From input data we choose the category software. We dont care for what software had specific comany what income but its total money raised and check how does it look like in histogram.

Plotting with Dask is quite a bit topic on itself, as input data can be much bigger than clients memory. For our case we can let Dask compute all tasks with DataFrame and then plot the result locally. In case this wouldn't be possible, we would have to use specific Dask methods and specify chunks etc.

<strong>Plot generated on cluster is almost never live! Usually sync computer puts together all the pieces and sends output graph as an image!</strong>
"""

# Calculating with previous DataFrame
# add .hist() at the end for local plot
df[df.category=="software"].groupby("company")["raisedAmt"].sum().reset_index().set_index("company").compute().hist()

"""<a id="4.5"></a>
## Custom lazy functions
Sometimes problems don’t fit into one of the collections like dask.array or dask.dataframe. Pretty much any function written in python can support lazy calculation with the use of dask.delayed interface. This allows us to create task graphs directly with a light annotation of any regular python code. Just make sure, that you call lazy computation on function and not its result (most common mistake).

#### First we define few regular functions
"""

# Define functions for increase, square and add
# In functions which should be lazy evaluated dont change the input directly! It could result in unexpected behavior.
def inc(x):
    # Do:
    x = x + 1
    # Don't:
    # x+=1
    return x

# Its possible to make function lazy evaluated by adding decorators
# @dask.delayed
def square(x):
    x = x**2
    return x

def add(x, y):
    z = x + y
    return z

"""With previously defined functions we want to perform following task over given data. We can clearly see in the code below, that as it is written right now, it will be solved sequentially in a single thread. However, we see that a lot of this could be executed in parallel."""

# sequential approach
data = [1, 2, 3, 4, 5]
output = []
for x in data:
    a = inc(x)
    b = square(x)
    c = add(a, b)
    output.append(c)

total = sum(output)
total

"""The Dask delayed function decorates your functions so that they operate lazily. Rather than executing your function immediately, it will defer execution, placing the function and its arguments into a task graph.

We will use the dask.delayed function to wrap the function calls that we want to turn into tasks. None of the <strong><em>inc, square, add</em></strong>, or <strong><em>sum</em></strong> calls will happen upon cell execution. Instead, the object <strong><em>total</em></strong> is a Delayed (class) result that contains a task graph of the entire computation.
"""

# dask approach
output = []

for x in data:
    a = dask.delayed(inc)(x)
    #a = dask.delayed(inc(x))
    b = dask.delayed(square)(x)
    c = dask.delayed(add)(a, b)
    output.append(c)

total = dask.delayed(sum)(output)

"""We can also visualize generated task graph for instance of a Delayed class. Looking at the graph we see clear opportunities for parallel execution. The Dask schedulers will exploit this parallelism, generally improving performance (although not in this example, because these functions are already very small and fast.)

#### Warning: Visialization is only possible with grahpviz python library and system library installed!
Python library can be installed through pip. Graphviz software can be installed from: https://graphviz.org/ Along with automatically added path also bin/dot.exe has to be added to system variables (windows)
"""

# Task graph
total.visualize()

"""And this task graph we can now calculate by calling compute method. Dask will perform all tasks in parallel.  """

# compute output
total.compute()

"""[top](#0)"""


import pandas as pd

df=pd.read_excel("test_data.xlsx")

dataselected=df.iloc[94263:95632,:]

dataselected.isnull().sum()

data_cl=dataselected.dropna()

data_cl=data_cl[(data_cl["Quality"]>=0) &(data_cl["Price"]>=0)]

data_cl["Invoice"]=data_cl["Invoice"].astype(str)

data_cl=data_cl[data_cl["Invoice"].str.isnumeric()]


data_cl["Invocie"].nuinque()

data_cl.groupby("Country")["Invoice"].nunique()

data_cl["TotalPrice"]=data_cl["Quant"]

data_cl.sort_values(by="TotalPrice",ascending=False)

data_cl.groupby("Customer")[["TotalPrices"]].sum().sort_values(by="TotalPrice",ascending=False)
data_cl[data_Cl["Custommer"]==26516]["Invoice"].count()





import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from scipy.spatial.distance import euclidean


df=pd.read_csv("data_test_clustering_A1.csv")
dataSelect=df.iloc[15161,651651,:]

pca=PCA(n_components=2)
pca_transform=pca.fit_transform(dataSelect)

pca_data=pd.DataFrame(data=pca_transform,columns=["PC1","PC2"])

plt.figure(figsize=(10,6))
plt.scatter(pca_data["PC1"],pca_data["PC2"],alpha=0.6)
plt.xlabel()
plt.ylabel()
plt.grid(True)
plt.show()


index11=pca_transform[11]
index832=pca_transform[832]

mostsig11=index11[0]
mostsig11=index832[1]

distance=euclidean(index11,index832)

kmeans=KMeans(n_clusters=4,random_state=0)
clusterlabel=kmeans.fit_predict(pca_transform)

cluster11=clusterlabel[11]
cluster832=clusterlabel[832]


local sum = 0
for i = 20, 80 do
  local value = redis.call('HGET', 'user:' .. i, 'a')
  if value then
    sum = sum + tonumber(value)
  end
end

return sum



